# -*- coding: utf-8 -*-
"""Untitled48.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zpVYBr8qnpJVPoab0LrI7j4beFNKG3V0
"""

import streamlit as st
import pandas as pd
import os
import re
from textblob import TextBlob
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
import gensim
from gensim.models import Word2Vec
from sklearn.metrics.pairwise import cosine_similarity

# Set Streamlit theme
st.set_page_config(
    page_title="WCO HS 2022 Explorer",
    page_icon=":globe_with_meridians:",
    layout="wide",
    initial_sidebar_state="expanded",
)

st.markdown(
    """
    <style>
    body {
        background-color: #d9d9d9;
        color: #0072cd;
    }
    .streamlit-expanderHeader {
        color: #0072ce !important;
    }
    .stButton>button {
        color: white;
        background-color: #0072ce;
        border: none;
    }
    .stTextInput>div>div>input {
        border: 2px solid #0072ce;
    }
    .dataframe {
        border: 2px solid #0072ce;
    }
    .dataframe th {
        background-color: #0072ce;
        color: white;
    }
    </style>
    """,
    unsafe_allow_html=True,
)

# 1. Data Loading
@st.cache_data
def load_data(file_path):
    try:
        df = pd.read_csv(file_path, low_memory=True)
        return df
    except Exception as e:
        st.error(f"Error loading data: {e}")
        st.stop()

repo_url = "https://github.com/datasets/harmonized-system.git"
repo_name = "harmonized-system"

if not os.path.exists(repo_name):
    os.system(f"git clone {repo_url}")

data_dir = f"./{repo_name}/data"
file_name = 'harmonized-system.csv'
file_path = os.path.join(data_dir, file_name)

df = load_data(file_path)
st.write("Data loaded successfully!")


# 2. Data Cleaning and Preprocessing
@st.cache_data
def preprocess_data(df):
    def clean_text(text):
        if isinstance(text, str):
            text = text.lower()
            text = re.sub(r'[^\w\s]', '', text)
            return text
        return ""

    if 'description' in df.columns:
        df['cleaned_description'] = df['description'].apply(clean_text)

    if 'section' in df.columns:
        section_counts = df['section'].value_counts()
        top_20_sections = section_counts.index[:20]
        df = df[df['section'].isin(top_20_sections)]

    if 'description' in df.columns:
        df['sentiment'] = df['description'].apply(lambda x: TextBlob(x).sentiment.polarity if isinstance(x, str) else 0)
    return df

df = preprocess_data(df)


# 3. Train Relevance Prediction Model
@st.cache_resource
def train_relevance_model(df):
    df['relevance'] = (df['sentiment'] + 1) / 2
    X = df[['sentiment']]
    y = df['relevance']
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    model = LinearRegression()
    model.fit(X_train, y_train)
    return model

model = train_relevance_model(df)


# 4. Train Custom Word Embedding Model
@st.cache_resource
def train_word_embedding(df):
    sentences = df['cleaned_description'].apply(lambda x: x.split()).tolist()
    custom_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)
    return custom_model

custom_model = train_word_embedding(df)


# --- Streamlit Search Functionality ---
def search_descriptions(search_term):
    search_term = search_term.lower()

    df['combined_text'] = df['cleaned_description'].astype(str) + ' ' + \
                           df['hscode'].astype(str) + ' ' + \
                           df['section'].astype(str)

    if search_term.isdigit():
        results = df[df['hscode'].astype(str).str.contains(search_term, na=False, regex=False)].copy()
        results['predicted_relevance'] = 1.0
    else:
        vectorizer = TfidfVectorizer()
        tfidf_matrix = vectorizer.fit_transform(df['combined_text'])
        search_vector = vectorizer.transform([search_term])

        cosine_similarities = cosine_similarity(search_vector, tfidf_matrix).flatten()
        df['relevance'] = cosine_similarities

        results = df[df['combined_text'].str.contains(search_term, na=False, regex=False)].copy()

        if not results.empty:
            results['predicted_relevance'] = model.predict(results[['sentiment']])
            results = results.sort_values(by='predicted_relevance', ascending=False)

    if results.empty:
        st.write(f"No results found for '{search_term}'.")
    else:
        if 'predicted_relevance' in results.columns:
            results['relevance_percent'] = (results['predicted_relevance'] / results['predicted_relevance'].max()) * 100
            st.dataframe(results[['hscode', 'description', 'section', 'relevance_percent']])
        else:
            st.write("No predicted relevance found for the search results.")


# --- Streamlit App ---
st.title("WCO HS 2022 Explorer")

search_term = st.text_input("Enter HS code (4 digits) or words")

if st.button("Search"):
    if search_term:
        search_descriptions(search_term)